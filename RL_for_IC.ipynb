{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC7fFQMcb69m"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM, CLIPModel, AutoProcessor, CLIPTokenizer, GPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# for image captioning\n",
    "# load a fine-tuned image captioning model and corresponding tokenizer and image processor\n",
    "ic_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "ic_tokenizer = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "ic_image_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# for CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ],
   "metadata": {
    "id": "EC6sS0ReeC4j",
    "ExecuteTime": {
     "end_time": "2023-10-16T04:22:11.725117400Z",
     "start_time": "2023-10-16T04:22:00.769597500Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpconnect/vit-gpt2-image-captioning were not used when initializing VisionEncoderDecoderModel: ['decoder.transformer.h.2.crossattention.masked_bias', 'decoder.transformer.h.0.crossattention.bias', 'decoder.transformer.h.6.attn.masked_bias', 'decoder.transformer.h.2.attn.bias', 'decoder.transformer.h.7.crossattention.masked_bias', 'decoder.transformer.h.3.crossattention.masked_bias', 'decoder.transformer.h.6.attn.bias', 'decoder.transformer.h.8.attn.bias', 'decoder.transformer.h.4.crossattention.masked_bias', 'decoder.transformer.h.3.attn.bias', 'decoder.transformer.h.5.attn.bias', 'decoder.transformer.h.5.crossattention.masked_bias', 'decoder.transformer.h.1.attn.masked_bias', 'decoder.transformer.h.10.crossattention.masked_bias', 'decoder.transformer.h.1.crossattention.bias', 'decoder.transformer.h.0.attn.masked_bias', 'decoder.transformer.h.9.crossattention.masked_bias', 'decoder.transformer.h.7.attn.bias', 'decoder.transformer.h.1.crossattention.masked_bias', 'decoder.transformer.h.11.attn.bias', 'decoder.transformer.h.9.crossattention.bias', 'decoder.transformer.h.8.attn.masked_bias', 'decoder.transformer.h.6.crossattention.masked_bias', 'decoder.transformer.h.11.attn.masked_bias', 'decoder.transformer.h.5.crossattention.bias', 'decoder.transformer.h.0.crossattention.masked_bias', 'decoder.transformer.h.4.attn.bias', 'decoder.transformer.h.0.attn.bias', 'decoder.transformer.h.2.attn.masked_bias', 'decoder.transformer.h.8.crossattention.masked_bias', 'decoder.transformer.h.2.crossattention.bias', 'decoder.transformer.h.8.crossattention.bias', 'decoder.transformer.h.4.attn.masked_bias', 'decoder.transformer.h.7.attn.masked_bias', 'decoder.transformer.h.11.crossattention.masked_bias', 'decoder.transformer.h.6.crossattention.bias', 'decoder.transformer.h.1.attn.bias', 'decoder.transformer.h.9.attn.masked_bias', 'decoder.transformer.h.10.attn.bias', 'decoder.transformer.h.11.crossattention.bias', 'decoder.transformer.h.10.crossattention.bias', 'decoder.transformer.h.4.crossattention.bias', 'decoder.transformer.h.9.attn.bias', 'decoder.transformer.h.7.crossattention.bias', 'decoder.transformer.h.3.attn.masked_bias', 'decoder.transformer.h.3.crossattention.bias', 'decoder.transformer.h.5.attn.masked_bias', 'decoder.transformer.h.10.attn.masked_bias']\n",
      "- This IS expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"https://farm8.staticflickr.com/7020/6810252887_01e3d8e4e6_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ],
   "metadata": {
    "id": "Atk42Vv0biUX",
    "ExecuteTime": {
     "end_time": "2023-10-16T04:22:12.868514200Z",
     "start_time": "2023-10-16T04:22:12.105287300Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def compute_loss(model, pixel_values, labels, sample_weights):\n",
    "    print(\"compute_loss\", flush=True)\n",
    "    output = model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    print(output.logits.size())\n",
    "    loss = criterion(\n",
    "        output.logits[:, :-1, :].reshape(-1, 50257),\n",
    "        labels[:, 1:].reshape(-1))\n",
    "\n",
    "    loss = loss.reshape(output.logits.size(dim=0), -1)\n",
    "    sample_weights = sample_weights.squeeze(0).unsqueeze(1).repeat(1, output.logits.size(dim=1) - 1)\n",
    "\n",
    "    print(loss.size(), sample_weights.size(), flush=True)\n",
    "    loss = loss * sample_weights\n",
    "    # loss.mean().backward()\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def compute_rl_loss(model, image_processor, tokenizer, img, gt_labels, reward_fct):\n",
    "    pixel_values = image_processor(img, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        pixel_values,\n",
    "        max_new_tokens=40,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences=3,\n",
    "    )\n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    print(\"Generated texts\")\n",
    "    print(generated_ids.size())\n",
    "    print(generated_texts, flush=True)\n",
    "\n",
    "    score = reward_fct(generated_texts, generated_ids, gt_labels, img)  # return tensor\n",
    "    mu = score.mean()\n",
    "    sample_weights = score - mu\n",
    "    print(score)\n",
    "    return compute_loss(model, pixel_values, generated_ids, sample_weights)"
   ],
   "metadata": {
    "id": "vj9JQJbBfrDB",
    "ExecuteTime": {
     "end_time": "2023-10-16T04:23:19.188109300Z",
     "start_time": "2023-10-16T04:23:19.139213800Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def compute_image_representation_from_image_instance(clip_processor, clip_model, image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs['pixel_values']\n",
    "    visual_outputs = clip_model.vision_model(pixel_values=pixel_values)\n",
    "    image_embeds = visual_outputs[1]\n",
    "    image_embeds = clip_model.visual_projection(image_embeds)  # [1 x embed_dim]\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "    return image_embeds\n",
    "\n",
    "\n",
    "def compute_image_text_similarity_via_embeddings(image_embeds, text_embeds, clip_model):\n",
    "    text_embeds = text_embeds.view(image_embeds.shape[0], -1, text_embeds.shape[-1])\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "    image_embeds = image_embeds.unsqueeze(-1)\n",
    "    logit_scale = clip_model.logit_scale.exp()\n",
    "    logits_per_text = torch.matmul(text_embeds, image_embeds) * logit_scale\n",
    "    logits_per_image = logits_per_text.squeeze(-1)\n",
    "    return logits_per_image.softmax(dim=1)  # , logits_per_image/logit_scale # batch x len(text_list)\n",
    "\n",
    "\n",
    "def compute_text_representation(text_list, clip_model, clip_tokenizer):\n",
    "    # text_list: a list of text\n",
    "    text_inputs = clip_tokenizer(text_list, padding=True, return_tensors=\"pt\",\n",
    "                                 max_length=clip_tokenizer.max_len_single_sentence + 2, truncation=True)\n",
    "    # self.tokenizer.max_len_single_sentence + 2 = 77\n",
    "    input_ids, attention_mask = text_inputs['input_ids'], text_inputs['attention_mask']\n",
    "\n",
    "    text_outputs = clip_model.text_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    text_embeds = text_outputs[1]\n",
    "    text_embeds = clip_model.text_projection(text_embeds)\n",
    "    return text_embeds\n",
    "\n",
    "\n",
    "def compute_image_text_similarity_via_raw_text(image_embeds, text_list, clip_model, clip_tokenizer):\n",
    "    text_embeds = compute_text_representation(text_list, clip_model, clip_tokenizer)\n",
    "    return compute_image_text_similarity_via_embeddings(image_embeds, text_embeds, clip_model)"
   ],
   "metadata": {
    "id": "k6mkuJyr8mlv",
    "ExecuteTime": {
     "end_time": "2023-10-16T04:22:24.543215600Z",
     "start_time": "2023-10-16T04:22:24.501280800Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def reward_clip(clip_model, clip_processor, clip_tokenizer):\n",
    "    def _reward_fct(generated_texts, generated_ids, gt_labels, img):\n",
    "        image_embeds = compute_image_representation_from_image_instance(clip_processor, clip_model, img)\n",
    "\n",
    "        return compute_image_text_similarity_via_raw_text(image_embeds, generated_texts, clip_model, clip_tokenizer)\n",
    "\n",
    "    return _reward_fct\n",
    "\n"
   ],
   "metadata": {
    "id": "_0JRp47_8ptM",
    "ExecuteTime": {
     "end_time": "2023-10-16T04:22:27.243053Z",
     "start_time": "2023-10-16T04:22:27.219257600Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "gt_labels = ic_tokenizer(\"a kitten plays with a colorful ball with a dog next to it.\", return_tensors=\"pt\")[\"input_ids\"]"
   ],
   "metadata": {
    "id": "djQEZeKqjTjZ",
    "ExecuteTime": {
     "end_time": "2023-10-16T04:22:28.959316300Z",
     "start_time": "2023-10-16T04:22:28.955315900Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss = compute_rl_loss(\n",
    "    ic_model, ic_image_processor, ic_tokenizer,\n",
    "    image, gt_labels,\n",
    "    reward_clip(clip_model, clip_processor, clip_tokenizer))"
   ],
   "metadata": {
    "id": "T7LWUlbT8z0K",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "95aef5ee-1928-473e-fd76-878b7fa2a3ba",
    "ExecuteTime": {
     "end_time": "2023-10-16T04:23:26.019093Z",
     "start_time": "2023-10-16T04:23:23.488316700Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated texts\n",
      "torch.Size([3, 13])\n",
      "['a dog and a cat playing with a ball ', 'a dog and a cat playing with a toy ', 'a dog and a cat are playing with a ball ']\n",
      "tensor([[0.2021, 0.0723, 0.7257]], grad_fn=<SoftmaxBackward0>)\n",
      "compute_loss\n",
      "torch.Size([3, 13, 50257])\n",
      "torch.Size([3, 12]) torch.Size([3, 12])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "loss"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ekXJnqgpSIm",
    "outputId": "c3b60fce-1afb-481a-f20c-ee75c40e7536",
    "ExecuteTime": {
     "end_time": "2023-10-16T04:22:33.092245200Z",
     "start_time": "2023-10-16T04:22:33.080006400Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.1059, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "IvYYD1fQnPED"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
